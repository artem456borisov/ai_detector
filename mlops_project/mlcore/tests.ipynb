{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc283e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data_correct.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14051742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca168d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "human_text      34713\n",
       "machine_text    27084\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe7f6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '- Strengths:\\n* Outperforms ALIGN in supervised entity linking task which suggests that the\\nproposed framework improves representations of text and knowledge that are\\nlearned jointly.\\n* Direct comparison with closely related approach using very similar input\\ndata.\\n* Analysis of the smoothing parameter provides useful analysis since impact of\\npopularity is a persistent issue in entity linking.\\n\\n- Weaknesses:\\n* Comparison with ALIGN could be better. ALIGN used content window size 10 vs\\nthis paper\\'s 5, vector dimension of 500 vs this paper\\'s 200. Also its not clear\\nto me whether N(e_j) includes only entities that link to e_j. The graph is\\ndirected and consists of wikipedia outlinks, but is adjacency defined as it\\nwould be for an undirected graph? For ALIGN, the context of an entity is the\\nset of entities that link to that entity. If N(e_j) is different, we cannot\\ntell how much impact this change has on the learned vectors, and this could\\ncontribute to the difference in scores on the entity similarity task. \\n* It is sometimes difficult to follow whether \"mention\" means a string type, or\\na particular mention in a particular document. The phrase \"mention embedding\"\\nis used, but it appears that embeddings are only learned for mention senses.\\n* It is difficult to determine the impact of sense disambiguation order without\\ncomparison to other unsupervised entity linking methods. \\n\\n- General Discussion:',\n",
       " 'label': 'human_text',\n",
       " 'domain': 'human'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49db9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import M4GTDataModule\n",
    "\n",
    "data_module = M4GTDataModule(\n",
    "    train_data_dir=  'data_correct.json',\n",
    "    val_data_dir= 'data_correct.json',\n",
    "    test_data_dir= 'data_correct.json',\n",
    "    predict_data_dir= 'data_correct.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df893785",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51cbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LigthningClassifier\n",
    "\n",
    "module = LigthningClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d988debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/Users/ruanwko/Library/Caches/pypoetry/virtualenvs/mlops-project-zy5VbVj4-py3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "trainer = L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fc860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruanwko/Library/Caches/pypoetry/virtualenvs/mlops-project-zy5VbVj4-py3.12/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\n",
      "  | Name  | Type            | Params | Mode  | FLOPs\n",
      "----------------------------------------------------------\n",
      "0 | model | TextsClassifier | 132 K  | train | 0    \n",
      "----------------------------------------------------------\n",
      "132 K     Trainable params\n",
      "0         Non-trainable params\n",
      "132 K     Total params\n",
      "0.530     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(module, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project-zy5VbVj4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
